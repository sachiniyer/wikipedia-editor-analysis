{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69385a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os, gc\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from lxml import etree as ET\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk/jre\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.12.0 pyspark-shell' \n",
    "num_of_th = 512; repartition_size = num_of_th*16; chunk_size = 5000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2025b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 20:47:04 WARN Utils: Your hostname, siyer resolves to a loopback address: 127.0.1.1; using 10.19.149.144 instead (on interface wlp166s0)\n",
      "23/04/13 20:47:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/siyer/.pyenv/versions/3.10.10/envs/mmds/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/siyer/.ivy2/cache\n",
      "The jars for the packages stored in: /home/siyer/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4f66025d-f809-4033-bde5-be028907472a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.12.0 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;2.3.3 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.2.5 in central\n",
      ":: resolution report :: resolve 96ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.12.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.2.5 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;2.3.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4f66025d-f809-4033-bde5-be028907472a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 20:47:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.19.149.144:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f03e9d04640>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"MyApp\") \\\n",
    "                  .set(\"spark.driver.memory\", \"8g\") \\\n",
    "                  .set(\"spark.executor.memory\", \"8g\")\n",
    "\n",
    "\n",
    "sc = SparkContext(master = \"local[*]\", conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "udf_registration = pyspark.sql.udf.UDFRegistration(spark)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfbdb7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_rdd = spark.read.text(\"./data/dump1.xml\", wholetext=False)\n",
    "\n",
    "file_chunk = file_rdd.take(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d309846d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 20:47:35 WARN TaskSetManager: Stage 3 contains a task of very large size (48755 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 312320,\n",
       " 1,\n",
       " 312320,\n",
       " 2,\n",
       " 312320,\n",
       " 3,\n",
       " 312320,\n",
       " 4,\n",
       " 312320,\n",
       " 5,\n",
       " 313344,\n",
       " 6,\n",
       " 312320,\n",
       " 7,\n",
       " 312320,\n",
       " 8,\n",
       " 312320,\n",
       " 9,\n",
       " 312320,\n",
       " 10,\n",
       " 313344,\n",
       " 11,\n",
       " 312320,\n",
       " 12,\n",
       " 312320,\n",
       " 13,\n",
       " 312320,\n",
       " 14,\n",
       " 312320,\n",
       " 15,\n",
       " 313152]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(file_chunk)\n",
    "\n",
    "def count_in_a_partition(idx, iterator):\n",
    "    count = 0\n",
    "    for _ in iterator:\n",
    "        count += 1\n",
    "    return idx,count\n",
    "\n",
    "count_list = rdd.mapPartitionsWithIndex(count_in_a_partition).collect()\n",
    "count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0cd539",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = {\n",
    "    \"TITLE_O\": \"<title\",\n",
    "    \"TEXT_O\": \"<text\",\n",
    "    \"IP_O\": \"<ip\",\n",
    "    \"USER_O\": \"<username\",\n",
    "    \"TITLE_C\": \"</title\",\n",
    "    \"TEXT_C\": \"</text\",\n",
    "    \"IP_C\": \"</ip\",\n",
    "    \"USER_C\": \"</username\",\n",
    "}\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"ip\", ArrayType(StringType()), True),\n",
    "    StructField(\"user\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "# Create an empty DataFrame with the specified schema\n",
    "\n",
    "entry_blank = {\n",
    "    \"title\": \"\",\n",
    "    \"text\": \"\",\n",
    "    \"ip\": [[]],\n",
    "    \"user\": [[]]\n",
    "}\n",
    "\n",
    "entry = entry_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c46fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "firehose = False\n",
    "info = True\n",
    "def log(val, level):\n",
    "    if level == -1 and firehose:\n",
    "        print(val)\n",
    "    if level == 0 and debug:\n",
    "        print(val)\n",
    "    if level == 1 and info:\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f81d53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_opening_tag(event):\n",
    "    if c[\"TITLE_O\"] in event:\n",
    "        return 1\n",
    "    elif c[\"TEXT_O\"] in event:\n",
    "        return 2\n",
    "    elif c[\"IP_O\"] in event:\n",
    "        return 3\n",
    "    elif c[\"USER_O\"] in event:\n",
    "        return 4\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def get_values(part):\n",
    "    entry_array = []\n",
    "    elem_set = 0\n",
    "    read = True\n",
    "    entry = entry_blank\n",
    "    \n",
    "    try:\n",
    "        cont = True\n",
    "        while(cont):\n",
    "            log(entry, -1)\n",
    "            if read:\n",
    "                event = next(part).value\n",
    "            match elem_set:\n",
    "                case 0:\n",
    "                    match return_opening_tag(event):\n",
    "                        case 1:\n",
    "                            log(\"title\", 0)\n",
    "                            if entry[\"title\"] != \"\" and entry[\"text\"] != \"\":\n",
    "                                entry_array.append(entry)\n",
    "                                entry = entry_blank\n",
    "                                gc.collect()\n",
    "                            elem_set = 1\n",
    "                            read = False\n",
    "                        case 2:\n",
    "                            log(\"text\", 0)\n",
    "                            elem_set = 2\n",
    "                            read = False\n",
    "                        case 3:\n",
    "                            log(\"ip\", 0)\n",
    "                            elem_set = 3\n",
    "                            read = False\n",
    "                        case 4:\n",
    "                            log(\"user\", 0)\n",
    "                            elem_set = 4\n",
    "                            read = False\n",
    "\n",
    "                case 1:\n",
    "                    read = True\n",
    "                    entry[\"title\"] += event\n",
    "                    if c[\"TITLE_C\"] in event:\n",
    "                        elem_set = 0\n",
    "\n",
    "                case 2:\n",
    "                    read = True\n",
    "                    entry[\"text\"] += event\n",
    "                    if c[\"TEXT_C\"] in event:\n",
    "                        elem_set = 0\n",
    "\n",
    "                case 3:\n",
    "                    read = True\n",
    "                    entry[\"ip\"][-1].append(event)\n",
    "                    if c[\"IP_C\"] in event:\n",
    "                        entry[\"ip\"].append([])\n",
    "                        elem_set = 0\n",
    "\n",
    "                case 4:\n",
    "                    read = True\n",
    "                    entry[\"user\"][-1].append(event)\n",
    "                    if c[\"USER_C\"] in event:\n",
    "                        entry[\"user\"].append([])\n",
    "                        elem_set = 0\n",
    "    except StopIteration:\n",
    "        cont = False\n",
    "    log(entry_array, -1)\n",
    "    return entry_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa2d9319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_values(part)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udf_registration.register(name=\"get_values\", f=get_values, returnType=ArrayType(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daff716b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 20:47:37 WARN TaskSetManager: Stage 4 contains a task of very large size (48755 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.               (0 + 16) / 16]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/siyer/.pyenv/versions/3.10.10/envs/mmds/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/siyer/.pyenv/versions/3.10.10/envs/mmds/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/siyer/.pyenv/versions/3.10.10/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m elems \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_,count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(count_list):\n\u001b[0;32m----> 3\u001b[0m     elems \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     log(\u001b[38;5;28mlen\u001b[39m(elems), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      5\u001b[0m     log(elems[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/mmds/lib/python3.10/site-packages/pyspark/context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1484\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1486\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/mmds/lib/python3.10/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/mmds/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/mmds/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "elems = []\n",
    "for idx_,count in enumerate(count_list):\n",
    "    elems = sc.runJob(rdd, lambda part: get_values(part))\n",
    "    log(len(elems), 1)\n",
    "    log(elems[0], 1)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e169be5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'elems' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43melems\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'elems' is not defined"
     ]
    }
   ],
   "source": [
    "len(elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "lda = LDA(k=100, seed=1, optimizer=\"em\")\n",
    "lda.setMaxIter(5)\n",
    "\n",
    "lda.getMaxIter()\n",
    "\n",
    "lda.clear(lda.maxIter)\n",
    "model = lda.fit(df)\n",
    "model.setSeed(1)\n",
    "\n",
    "model.getTopicDistributionCol()\n",
    "\n",
    "model.isDistributed()\n",
    "\n",
    "localModel = model.toLocal()\n",
    "localModel.isDistributed()\n",
    "\n",
    "model.vocabSize()\n",
    "\n",
    "model.describeTopics().show()\n",
    "\n",
    "model.topicsMatrix()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
