{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os, gc\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, ArrayType, StringType\n",
    "import xml.etree.ElementTree as ET\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.15.0,org.apache.hadoop:hadoop-core:1.2.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .config(\"spark.driver.memory\", \"32g\") \\\n",
    "        .master(\"spark://cm013:47322\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\"title\", \"text\", \"ip\", \"id\"]\n",
    "df = spark.read.format(\"com.databricks.spark.xml\") \\\n",
    "        .option(\"rootTag\", \"mediawiki\") \\\n",
    "        .option(\"rowTag\", \"page\") \\\n",
    "        .option(\"excludeAttribute\", True) \\\n",
    "        .load(\"dump.xml\") \\\n",
    "        .select(*tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = \"small_dump.xml\"\n",
    "xmlDF = spark.read \\\n",
    "    .format(\"com.databricks.spark.xml\") \\\n",
    "    .option(\"rootTag\", \"root\") \\\n",
    "    .option(\"rowTag\", \"page\") \\\n",
    "    .load(inputFile)\n",
    "\n",
    "xmlDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, explode, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "def parse_xml(xml_string):\n",
    "    print(\"hello\")\n",
    "    root = ET.fromstring(xml_string)\n",
    "    print(xml_string)\n",
    "    return [(\n",
    "        child.findtext(\"title\"),\n",
    "        child.findtext(\"text\")\n",
    "    ) for child in root.iter(\"page\")]\n",
    "\n",
    "parse_xml_udf = udf(parse_xml, ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"text\", StringType(), True)\n",
    "    ])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"xml\") \\\n",
    "    .option(\"rootTag\", \"mediawiki\") \\\n",
    "    .option(\"rowTag\", \"page\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .load(\"dump.xml\") \\\n",
    "    .withColumn(\"page\", explode(expr(\"split(xml, '</page>')\"))) \\\n",
    "    .selectExpr(\"xpath_string(page, 'page/title/text()') AS title\",\n",
    "                \"xpath_string(page, 'page/id/text()') AS id\",\n",
    "                \"xpath_string(page, 'page/revision/contributor/username/text()') AS contributor_username\",\n",
    "                \"xpath_string(page, 'page/revision/contributor/id/text()') AS contributor_id\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove(\"remp.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"xml\") \\\n",
    "    .option(\"rootTag\", \"mediawiki\") \\\n",
    "    .option(\"rowTag\", \"page\") \\\n",
    "    .option(\"xpath\", \"concat(substring-before(., '</page>'), '</page>') AS xml\") \\\n",
    "    .load(\"dump.xml\") \\\n",
    "    .repartition(10)\n",
    "\n",
    "parse_xml_udf = udf(lambda x: parse_xml(x, tags_to_extract), tags_schema)\n",
    "\n",
    "df = df.selectExpr(\"explode(split(xml, '</page>)')) as page\")\n",
    "\n",
    "df = df.selectExpr(\"page AS xml\").select(parse_xml_udf(\"xml\").alias(\"page\")).select(\"page.*\")\n",
    "\n",
    "tags_to_extract=[\"id\", \"ip\", \"title\"]\n",
    "df = df.select([c for c in df.columns if c in tags_to_extract])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rdd = spark.read.text(\"dump.xml\")\n",
    "df = spark.read.format(\"com.databricks.spark.xml\").option(\"rowTag\", \"page\").option(\"wholeFile\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xml_rdd = spark.sparkContext.newAPIHadoopFile(\n",
    "    'dump.xml',\n",
    "    'com.databricks.spark.xml.XmlInputFormat',\n",
    "    'org.apache.hadoop.io.LongWritable',\n",
    "    'org.apache.hadoop.io.Text',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_tree(element, data):\n",
    "    if element.tag == \"title\":\n",
    "        data[0] = element.text\n",
    "    elif element.tag == \"text\":\n",
    "        data[1] = element.text\n",
    "    elif element.tag == \"ip\":\n",
    "        data[2].append(element.text)\n",
    "    elif element.tag == \"username\":\n",
    "        data[3].append(element.text)\n",
    "    for child in element:\n",
    "        data = iterate_tree(child, data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('text', StringType(), True),\n",
    "    StructField('ips', ArrayType(StringType()), True),\n",
    "    StructField('usernames', ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "\n",
    "parsed_rdd = xml_rdd.map(lambda x: ET.fromstring(x[1])) \\\n",
    "    .map(lambda x: iterate_tree(x, [\"\",\"\",[],[]])) \\\n",
    "    .map(lambda x: tuple(x))\n",
    "\n",
    "df = spark.createDataFrame(parsed_rdd, schema)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"XML to RDD\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"output.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2][\"{http://www.mediawiki.org/xml/export-0.10/}text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
